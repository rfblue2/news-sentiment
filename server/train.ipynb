{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import paddle.v2 as paddle\n",
    "\n",
    "# PaddlePaddle init\n",
    "paddle.init(use_gpu=True, trainer_count=1, log_error_clipping=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_handler(event):\n",
    "    if isinstance(event, paddle.event.EndIteration):\n",
    "        if event.batch_id % 100 == 0:\n",
    "            print \"\\nPass %d, Batch %d, Cost %f, %s\" % (\n",
    "                event.pass_id, event.batch_id, event.cost, event.metrics)\n",
    "        else:\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "    if isinstance(event, paddle.event.EndPass):\n",
    "        with open('./params_pass_%d.tar' % event.pass_id, 'w') as f:\n",
    "                parameters.to_tar(f)\n",
    "\n",
    "        result = trainer.test(reader=test_reader, feeding=feeding)\n",
    "        print \"\\nTest with Pass %d, %s\" % (event.pass_id, result.metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As alluded to in section [Model Overview](#model-overview), here we provide the implementations of both Text CNN and Stacked-bidirectional LSTM models.\n",
    "\n",
    "### Text Convolution Neural Network (Text CNN)\n",
    "\n",
    "We create a neural network `convolution_net` as the following snippet code.\n",
    "\n",
    "Note: `paddle.networks.sequence_conv_pool` includes both convolution and pooling layer operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_net(input_dim, class_dim=2, emb_dim=128, hid_dim=128):\n",
    "    data = paddle.layer.data(\"word\",\n",
    "                             paddle.data_type.integer_value_sequence(input_dim))\n",
    "    emb = paddle.layer.embedding(input=data, size=emb_dim)\n",
    "    conv_3 = paddle.networks.sequence_conv_pool(\n",
    "        input=emb, context_len=3, hidden_size=hid_dim)\n",
    "    conv_4 = paddle.networks.sequence_conv_pool(\n",
    "        input=emb, context_len=4, hidden_size=hid_dim)\n",
    "    output = paddle.layer.fc(input=[conv_3, conv_4],\n",
    "                             size=class_dim,\n",
    "                             act=paddle.activation.Softmax())\n",
    "    lbl = paddle.layer.data(\"label\", paddle.data_type.integer_value(2))\n",
    "    cost = paddle.layer.classification_cost(input=output, label=lbl)\n",
    "    return cost, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Define input data and its dimension\n",
    "\n",
    "    Parameter `input_dim` denotes the dictionary size, and `class_dim` is the number of categories. In `convolution_net`, the input to the network is defined in `paddle.layer.data`.\n",
    "\n",
    "1. Define Classifier\n",
    "\n",
    "    The above Text CNN network extracts high-level features and maps them to a vector of the same size as the categories. `paddle.activation.Softmax` function or classifier is then used for calculating the probability of the sentence belonging to each category.\n",
    "\n",
    "1. Define Loss Function\n",
    "\n",
    "    In the context of supervised learning, labels of the training set are defined in `paddle.layer.data`, too. During training, cross-entropy is used as loss function in `paddle.layer.classification_cost` and as the output of the network; During testing, the outputs are the probabilities calculated in the classifier.\n",
    "\n",
    "#### Stacked bidirectional LSTM\n",
    "\n",
    "We create a neural network `stacked_lstm_net` as below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stacked_lstm_net(input_dim,\n",
    "                     class_dim=2,\n",
    "                     emb_dim=128,\n",
    "                     hid_dim=512,\n",
    "                     stacked_num=3):\n",
    "    \"\"\"\n",
    "    A Wrapper for sentiment classification task.\n",
    "    This network uses a bi-directional recurrent network,\n",
    "    consisting of three LSTM layers. This configuration is\n",
    "    motivated from the following paper, but uses few layers.\n",
    "        http://www.aclweb.org/anthology/P15-1109\n",
    "    input_dim: here is word dictionary dimension.\n",
    "    class_dim: number of categories.\n",
    "    emb_dim: dimension of word embedding.\n",
    "    hid_dim: dimension of hidden layer.\n",
    "    stacked_num: number of stacked lstm-hidden layer.\n",
    "    \"\"\"\n",
    "    assert stacked_num % 2 == 1\n",
    "\n",
    "    fc_para_attr = paddle.attr.Param(learning_rate=1e-3)\n",
    "    lstm_para_attr = paddle.attr.Param(initial_std=0., learning_rate=1.)\n",
    "    para_attr = [fc_para_attr, lstm_para_attr]\n",
    "    bias_attr = paddle.attr.Param(initial_std=0., l2_rate=0.)\n",
    "    relu = paddle.activation.Relu()\n",
    "    linear = paddle.activation.Linear()\n",
    "\n",
    "    data = paddle.layer.data(\"word\",\n",
    "                             paddle.data_type.integer_value_sequence(input_dim))\n",
    "    emb = paddle.layer.embedding(input=data, size=emb_dim)\n",
    "\n",
    "    fc1 = paddle.layer.fc(input=emb,\n",
    "                          size=hid_dim,\n",
    "                          act=linear,\n",
    "                          bias_attr=bias_attr)\n",
    "    lstm1 = paddle.layer.lstmemory(\n",
    "        input=fc1, act=relu, bias_attr=bias_attr)\n",
    "\n",
    "    inputs = [fc1, lstm1]\n",
    "    for i in range(2, stacked_num + 1):\n",
    "        fc = paddle.layer.fc(input=inputs,\n",
    "                             size=hid_dim,\n",
    "                             act=linear,\n",
    "                             param_attr=para_attr,\n",
    "                             bias_attr=bias_attr)\n",
    "        lstm = paddle.layer.lstmemory(\n",
    "            input=fc,\n",
    "            reverse=(i % 2) == 0,\n",
    "            act=relu,\n",
    "            bias_attr=bias_attr)\n",
    "        inputs = [fc, lstm]\n",
    "\n",
    "    fc_last = paddle.layer.pooling(\n",
    "        input=inputs[0], pooling_type=paddle.pooling.Max())\n",
    "    lstm_last = paddle.layer.pooling(\n",
    "        input=inputs[1], pooling_type=paddle.pooling.Max())\n",
    "    output = paddle.layer.fc(input=[fc_last, lstm_last],\n",
    "                             size=class_dim,\n",
    "                             act=paddle.activation.Softmax(),\n",
    "                             bias_attr=bias_attr,\n",
    "                             param_attr=para_attr)\n",
    "\n",
    "    lbl = paddle.layer.data(\"label\", paddle.data_type.integer_value(2))\n",
    "    cost = paddle.layer.classification_cost(input=output, label=lbl)\n",
    "    return cost, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Define input data and its dimension\n",
    "\n",
    "    Parameter `input_dim` denotes the dictionary size, and `class_dim` is the number of categories. In `stacked_lstm_net`, the input to the network is defined in `paddle.layer.data`.\n",
    "\n",
    "1. Define Classifier\n",
    "\n",
    "    The above stacked bidirectional LSTM network extracts high-level features and maps them to a vector of the same size as the categories. `paddle.activation.Softmax` function or classifier is then used for calculating the probability of the sentence belonging to each category.\n",
    "\n",
    "1. Define Loss Function\n",
    "\n",
    "    In the context of supervised learning, labels of the training set are defined in `paddle.layer.data`, too. During training, cross-entropy is used as loss function in `paddle.layer.classification_cost` and as the output of the network; During testing, the outputs are the probabilities calculated in the classifier.\n",
    "\n",
    "\n",
    "To reiterate, we can either invoke `convolution_net` or `stacked_lstm_net`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.urlretrieve(url, fname)\n",
    "\n",
    "# uncomment if redownloading is needed:\n",
    "# download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "# download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['rev_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels a comment as an attack if the majority of annotators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def strip_comment_to_word_list(comment):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        return re.sub(\"(@[A-Za-z0-9]+)|([^'0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", comment).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_word_dict = {}\n",
    "incr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_examples = []\n",
    "for comment in comments[\"comment\"].values:\n",
    "    ls = strip_comment_to_word_list(comment)\n",
    "    for word in ls:\n",
    "        if word not in comment_word_dict:\n",
    "            incr += 1\n",
    "            comment_word_dict[word] = incr\n",
    "    wiki_examples.append(map(lambda x: comment_word_dict[x], ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples = len(wiki_examples)\n",
    "num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word_dict = paddle.dataset.imdb.word_dict()\n",
    "word_dict = comment_word_dict\n",
    "\n",
    "dict_dim = len(word_dict)\n",
    "class_dim = 2\n",
    "\n",
    "# option 1\n",
    "[cost, output] = convolution_net(dict_dim, class_dim=class_dim)\n",
    "# option 2\n",
    "# [cost, output] = stacked_lstm_net(dict_dim, class_dim=class_dim, stacked_num=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training\n",
    "\n",
    "### Define Parameters\n",
    "\n",
    "First, we create the model parameters according to the previous model configuration `cost`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create parameters\n",
    "parameters = paddle.parameters.create(cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create Trainer\n",
    "\n",
    "Before jumping into creating a training module, algorithm setting is also necessary.\n",
    "Here we specified `Adam` optimization algorithm via `paddle.optimizer`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "adam_optimizer = paddle.optimizer.Adam(\n",
    "    learning_rate=2e-3,\n",
    "    regularization=paddle.optimizer.L2Regularization(rate=8e-4),\n",
    "    model_average=paddle.optimizer.ModelAverage(average_window=0.5))\n",
    "\n",
    "# create trainer\n",
    "trainer = paddle.trainer.SGD(cost=cost,\n",
    "                                parameters=parameters,\n",
    "                                update_equation=adam_optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training\n",
    "\n",
    "`paddle.dataset.imdb.train()` will yield records during each pass, after shuffling, a batch input is generated for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def custom_wiki_reader():\n",
    "    while True:\n",
    "        idx = random.randint(0, num_examples - 1)\n",
    "        yield (wiki_examples[idx], int(labels.values[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_reader = paddle.batch(\n",
    "    paddle.reader.shuffle(\n",
    "        lambda: paddle.dataset.imdb.train(word_dict), buf_size=1000),\n",
    "    batch_size=100)\n",
    "\n",
    "test_reader = paddle.batch(\n",
    "    lambda: paddle.dataset.imdb.test(word_dict), batch_size=100)\n",
    "\n",
    "# ---- let's overwrite the above\n",
    "\n",
    "train_reader = paddle.batch(paddle.reader.shuffle(lambda: custom_wiki_reader(), buf_size=1000), batch_size=25)\n",
    "\n",
    "test_reader = paddle.batch(custom_wiki_reader(), batch_size=10)\n",
    "# i know this is committing ML heresy by not separating train and test data, but this isn't a research project..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`feeding` is devoted to specifying the correspondence between each yield record and `paddle.layer.data`. For instance, the first column of data generated by `paddle.dataset.imdb.train()` corresponds to `word` feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feeding = {'word': 0, 'label': 1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Callback function `event_handler` will be invoked to track training progress when a pre-defined event happens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we can invoke `trainer.train` to start training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pass 0, Batch 0, Cost 0.692342, {'classification_error_evaluator': 0.3199999928474426}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 100, Cost 0.692865, {'classification_error_evaluator': 0.36000001430511475}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 200, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 300, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 400, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 500, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 600, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 700, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 800, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 900, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 1000, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 1100, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 1200, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 1300, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 1400, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 1500, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 1600, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 1700, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 1800, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 1900, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 2000, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 2100, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 2200, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 2300, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 2400, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 2500, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 2600, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 2700, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 2800, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 2900, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 3000, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 3100, Cost nan, {'classification_error_evaluator': 1.0}\n",
      "...................................................................................................\n",
      "Pass 0, Batch 3200, Cost nan, {'classification_error_evaluator': 1.0}\n",
      ".."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dceee64c00cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mevent_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevent_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfeeding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeeding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     num_passes=10)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/paddle/v2/trainer.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, reader, num_passes, event_handler, feeding)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__prepare_parameter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 self.__gradient_machine__.forwardBackward(in_args, out_args,\n\u001b[0;32m--> 164\u001b[0;31m                                                           pass_type)\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__gradient_machine__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpass_evaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__gradient_machine__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_evaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py_paddle/util.pyc\u001b[0m in \u001b[0;36mforwardBackward\u001b[0;34m(self, inArgs, outArgs, passType, callback)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \"\"\"\n\u001b[1;32m    192\u001b[0m         self.__forwardBackward__(inArgs, outArgs, passType,\n\u001b[0;32m--> 193\u001b[0;31m                                  __ParameterCallbackWrapper__.wrap(callback))\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mswig_paddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientMachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardBackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforwardBackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py_paddle/swig_paddle.pyc\u001b[0m in \u001b[0;36mforwardBackward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         \u001b[0mforwardBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradientMachine\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArguments\u001b[0m \u001b[0minArgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArguments\u001b[0m \u001b[0moutArgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0menumeration_wrapper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPassType\u001b[0m \u001b[0mpassType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \"\"\"\n\u001b[0;32m-> 2069\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swig_paddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientMachine_forwardBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    reader=train_reader,\n",
    "    event_handler=event_handler,\n",
    "    feeding=feeding,\n",
    "    num_passes=10)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
